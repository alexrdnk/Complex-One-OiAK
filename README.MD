# Porównanie różnych reprezentacji liczb zespolonych – analiza projektu

## 1. Przegląd systemów reprezentacji liczb zespolonych

W projektowanej analizie porównujemy kilka sposobów reprezentacji liczb zespolonych w systemach pozycyjnych o nietypowych podstawach oraz w klasycznej notacji binarnej. Rozważane reprezentacje to:

- **Radix-(2j)** – system o podstawie $2j$ ( gdzie $j = \sqrt{-1}$, jednostka urojona), zwany **quater-imaginary** (ćwierć-urojony) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=As%20early%20as%201960%2C%20Knuth,2j%29%20which)).
- **Radix-(j-1)** – system o podstawie $j-1$ (co odpowiada $-1 + j$), bazujący na pomyśle reprezentacji zespolonej z użyciem cyfr binarnych ([](https://tomrocksmaths.com/wp-content/uploads/2020/05/benjamin-chung.pdf#:~:text=There%20are%20two%20complex%20bases,every%20complex%20number%20that%20it)).
- **Radix-(j√2)** – system o podstawie $j\sqrt{2}$ (czyli $i\sqrt{2}$), określany czasem jako **bi-urojony** (podwójnie urojony) w literaturze ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=computations%20in%20the%20radix,j%20f%20i)).
- **Klasyczna reprezentacja binarna** – przechowywanie części rzeczywistej i urojonej oddzielnie w tradycyjnym systemie dwójkowym (z wykorzystaniem uzupełnienia do 2 dla liczb ze znakiem).

Poniżej przedstawiono zasadę działania każdej reprezentacji, wraz z jej zaletami i ograniczeniami.

### Reprezentacja radix-(2j) (podstawa $2i$)

**Zasada działania:** W systemie o podstawie $2j$ każda liczba zespolona zapisywana jest jako ciąg cyfr (pozycyjnie) przy podstawie $2j$. Przez analogię do systemu czwórkowego (radix-4) system ten bywa nazywany *quater-imaginary* ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=As%20early%20as%201960%2C%20Knuth,2j%29%20which)). Podstawa jest liczbą urojoną $2j$, zatem kolejne pozycje w zapisie oznaczają potęgi $...(2j)^2, (2j)^1, (2j)^0, (2j)^{-1}...$ itd. Zauważmy, że $(2j)^2 = -4$, co powoduje przemieszanie części rzeczywistej i urojonej przy kolejnych wagach. W tym systemie używa się **czterech cyfr: 0, 1, 2, 3** – to wystarcza, by unikatowo przedstawić *każdą* liczbę zespoloną bez konieczności stosowania osobnego znaku minus ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=As%20early%20as%201960%2C%20Knuth,2j%29%20which)). Przykładowo, w tym systemie liczba $-3 - 8j$ może mieć zapis $(1101)_{2j}$, co oznacza $1\cdot(2j)^3 + 1\cdot(2j)^2 + 0\cdot(2j)^1 + 1\cdot(2j)^0 = -8j - 4 + 1$ ([Quater-imaginary base - Wikipedia](https://en.wikipedia.org/wiki/Quater-imaginary_base#:~:text=To%20convert%20the%20string%20Image%3A,fill%20in%20the%20formula%20above)). 

Knuth zauważył, że system radix-$(2j)$ ma *ciekawą własność*: **mnożenie i dzielenie** liczb zespolonych można wykonać analogicznie do innych systemów pozycyjnych, stosując jedynie zmodyfikowane reguły przenoszenia ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=the%20conversion%20to%20and%20from,1%20two)). Innymi słowy, można mnożyć dwa liczby zapisane w radix-$(2j)$ kolumna po kolumnie (cyfra razy cyfra) – podobnie jak mnożenie pisemne – ale gdy suma w danej kolumnie przekroczy 3 (bo **podstawa $2j$ odpowiada czterem cyfrom 0–3**), wówczas odejmuje się 4 od tej sumy i **przenosi** $-1$ do **dwie** pozycje w lewo (ponieważ $(2j)^1$ razy $(2j)^1$ daje $(2j)^2 = -4$, co odpowiada przesunięciu o dwie pozycje) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=allows%20the%20multiplication%20and%20division,two%20columns%20to%20the%20left)). Analogicznie, jeśli w kolumnie wynik pojawi się wartość ujemna, dodaje się 4 i przenosi $+1$ dwie kolumny w lewo ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=allows%20the%20multiplication%20and%20division,two%20columns%20to%20the%20left)). Dzięki temu operacje dodawania czy mnożenia mogą przebiegać **w sposób jednolity**, bez potrzeby oddzielnego traktowania części rzeczywistych i urojonych podczas samego algorytmu – różnice tkwią tylko w regułach przenoszenia cyfr.

**Ograniczenia:** Mimo braku znaku i zjednoczenia reprezentacji, system $2j$ ma swoje wady. Po pierwsze, wymaga on czterech różnych cyfr (większy alfabet cyfrowy niż system binarny), co komplikuje implementację na sprzęcie binarnym. Po drugie, reprezentacja w podstawie $2j$ okazuje się równoważna reprezentowaniu części rzeczywistej i urojonej w **bazie $-4$** ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Representing%20complex%20numbers%20in%20radix,4)). Użycie ujemnej podstawy ($-4$) powoduje, że zakres liczb reprezentowalnych przy stałej długości słowa jest **niezrównoważony** – maksymalne liczby dodatnie i minimalne ujemne nie mają symetrycznych wartości bezwzględnych ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Representing%20complex%20numbers%20in%20radix,4)). Innymi słowy, dla ustalonej liczby cyfr zakres reprezentacji nie jest symetryczny wokół zera, co komplikuje wykorzystanie pełnej precyzji i kontrolę nad przekroczeniem zakresu ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Further%2C%20many%20of%20the%20alternative,these%20alternative%20complex%20radices%20follows)). W analizach wykazano, że **niezredundantna** (tj. niepozwalająca na więcej niż jeden zapis danej liczby) wersja systemu radix-$(2j)$ cierpi na kilka takich niekorzystnych własności i **nie daje przewagi** w implementacjach sprzętowych nad klasycznym systemem binarnym ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=sum%2C%20and%20c%2C%20is%20the,and%20imaginary%20parts%20treated%20separately)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20summary%2C%20this%20analysis%20shows,a%20conventional%20binary%20number%20system)). Co więcej, próby poprawy (np. przez wprowadzenie tzw. *redundancji* – dopuszczenie szerszego zbioru cyfr pomocniczych) sprowadzają się do reprezentacji równoważnej radix-4 z osobnym traktowaniem części rzeczywistej i urojonej, niwelując oryginalne założenia ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=sum%2C%20and%20c%2C%20is%20the,and%20imaginary%20parts%20treated%20separately)).

### Reprezentacja radix-(j-1) (podstawa $j-1$ czyli $-1 + i$)

**Zasada działania:** Reprezentacja o podstawie $j-1$ została zaproponowana przez Penneya (1965) jako kompleksowy odpowiednik systemu binarnego ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=have%20been%20proposed.%20Knuth%20,1.%20Slekys%20%5B5%5D%20defined)) ([](https://tomrocksmaths.com/wp-content/uploads/2020/05/benjamin-chung.pdf#:~:text=There%20are%20two%20complex%20bases,every%20complex%20number%20that%20it)). Podstawa $\rho = j-1$ jest liczbą zespoloną o wartości $-1 + i$ (przy czym $|j-1| = \sqrt{2}$). System ten jest o tyle atrakcyjny, że do zapisu liczb **wystarczają cyfry 0 i 1**, a więc **każda liczba zespolona może być zapisana w postaci binarnej** przy tej podstawie ([](https://tomrocksmaths.com/wp-content/uploads/2020/05/benjamin-chung.pdf#:~:text=There%20are%20two%20complex%20bases,every%20complex%20number%20that%20it)). Działa to podobnie jak w klasycznym systemie dwójkowym, z tą różnicą, że wagi kolejnych bitów są potęgami liczby $-1+i$. Przykładowo, kolejne potęgi $(-1+i)$ (odpowiadające kolejnym pozycjom bitowym) to: $(j-1)^0 = 1$, $(j-1)^1 = -1 + i$, $(j-1)^2 = i( -1 + i) = -2i$ (co daje $-2j$, czysto urojony), $(j-1)^3 = (-1+i)^2 \cdot (-1+i) = -2i \cdot (-1+i) = 2 - 2i$, $(j-1)^4 = (-1+i)^3\cdot(-1+i) = (2-2i)(-1+i) = -4$ i tak dalej ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=example%2C%20if%20N%20is%20a,left%2C%20in%20groups%20of%20four)). Widać, że potęgi tej podstawy przyjmują dość zróżnicowane wartości (naprzemiennie rzeczywiste, urojone, mieszane). Każdy bit w liczbie reprezentuje pewną kombinację jednostek rzeczywistej i urojonej. Mimo to **każdą** liczbę (np. każdą parę całkowitych $(A,B)$) można przedstawić jako skończony ciąg bitów 0/1 przy podstawie $-1+i$ – system ten pokrywa wszystkie całkowite punkty siatki Gaussa (tzn. wszystkie liczby $A + B i$ dla całkowitych $A, B$) w sposób jednoznaczny ([](https://tomrocksmaths.com/wp-content/uploads/2020/05/benjamin-chung.pdf#:~:text=There%20are%20two%20complex%20bases,every%20complex%20number%20that%20it)).

**Ograniczenia:** Głównym wyzwaniem w tym systemie jest **propagacja przeniesień przy dodawaniu bitów**. Ponieważ podstawa $-1+i$ ma część rzeczywistą i urojoną, dodanie dwóch liczb binarnych w tej reprezentacji może skutkować pojawieniem się przeniesienia **nie o jedną pozycję (bit) w lewo, lecz o dwie pozycje** lub więcej. Dzieje się tak, gdyż $(j-1)^2 = -2j$ i ewentualny nadmiar (np. sumy bitów przekraczający 1) skutkuje przeniesieniem składowych do pozycji odpowiadającej $(j-1)^2$, czyli o dwa miejsca w lewo ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=%28l%29,l%2B%2C%20%2817)). W praktyce oznacza to, że **przy dodawaniu** dwóch liczb w systemie radix-$(j-1)$ może powstać złożona kaskada przeniesień: jeden bit wynikowy może wpływać na dwa kolejne po lewej, te z kolei mogą wygenerować przeniesienia do jeszcze dalszych pozycji itd. ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,shown%20in%20the%20Table%20I)). Taka sytuacja jest znacznie bardziej skomplikowana niż w zwykłym binarnym dodawaniu (gdzie przeniesienie przechodzi najwyżej o jedno miejsce naraz). W literaturze wskazuje się, że *problem propagacji przeniesień* w radix-$(j-1)$ znacznie komplikuje zarówno dodawanie, jak i sumowanie częściowych iloczynów w mnożeniu ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,additional%20latency%20and%20gate%20complexity)). Aby zrealizować dodawanie bez blokowania całej równoległej maszyny arytmetycznej, potrzebne są specjalne układy do **wykrywania zer** (tzw. *zero-detectors*) dla każdej cyfry (bitu) – pozwalają one przewidzieć i obsłużyć przeniesienia skaczące o kilka pozycji ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,additional%20latency%20and%20gate%20complexity)). Takie dodatkowe układy zwiększają **złożoność sprzętową i opóźnienie** operacji (więcej bramek logicznych i większa latencja sygnału) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=and%20Blest%27s%20work%20,requirement%20for%20the%20addition%20operation)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,additional%20latency%20and%20gate%20complexity)). Innymi słowy, choć reprezentacja $j-1$ jest czysto binarna co do cyfr, to **implementacja sumatora czy mnożenia** dla niej jest bardziej złożona niż dla zwykłych liczb na dwójkowych (2’s complement) – może wymagać znacznie więcej bramek i czasu propagacji sygnału w układzie cyfrowym ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=procedure,requirement%20for%20the%20addition%20operation)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=and%20Blest%27s%20work%20,requirement%20for%20the%20addition%20operation)). W kontekście programowym oznacza to bardziej skomplikowane algorytmy dodawania/mnożenia, uwzględniające nietypowe przeniesienia. 

Warto zauważyć, że konwersja liczby klasycznej (dwóch 2’s complement) na zapis w systemie $j-1$ sprowadza się do konwersji osobno części rzeczywistej i urojonej z base-2 na base-$-4$ (czyli na system z ujemną czwórkową podstawą), a następnie wykonania pojedynczego dodawania w systemie $(j-1)$ ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=using%20the%20base%20%28j,Hardware%20implementations%20were%20not)). To wskazuje znów, że sednem jest operowanie na składowych w bazie -4 i dodatkowe operacje łączące, co niekoniecznie jest prostsze niż konwencjonalne metody.

### Reprezentacja radix-(j√2) (podstawa $j\sqrt{2}$)

**Zasada działania:** System o podstawie $j\sqrt{2}$ (gdzie $\rho = i\sqrt{2}$) to kolejny kompleksowy system binarny analizowany m.in. przez Slekysa (1964) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=computations%20in%20the%20radix,j%20f%20i)). Podstawa ta ma moduł $|\rho| = \sqrt{2}$ i kierunek czysto urojony (tj. $\rho$ jest liczbą urojoną $i$ skalowaną $\sqrt{2}$). Podobnie jak w przypadku $j-1$, teoretycznie **cyfry 0 i 1** wystarczają do zapisu liczb w tym systemie. Niestety okazuje się, że ten wybór podstawy ma pewną fundamentalną wadę: **nawet proste liczby zespolone mogą wymagać nieskończonego rozwinięcia**. Przykładowo, liczba $0 + 1j$ (czyli $i$) **nie ma skończonego przedstawienia w systemie o podstawie $j\sqrt{2}$** – jej zapis w tych jednostkach byłby nieskończony i nieokresowy ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=B.%20Radix,uses%20only%20the%20digits%200)). Oznacza to, że nie da się wyznaczyć skończonej sekwencji bitów 0/1 przy podstawie $i\sqrt{2}$, która dokładnie równa się $i$. Ten problem przypomina istnienie ułamków nieskończonych w systemach dziesiętnych (np. 1/3 = 0.333... w dziesiętnym). W efekcie, **system $j\sqrt{2}$ nie jest w pełni praktyczny do reprezentacji wszystkich liczb zespolonych z ograniczoną precyzją** – pewne wartości wymagałyby nieskończonej liczby cyfr lub aproksymacji.

W odpowiedzi na ten problem Slekys zaproponował tzw. *zmodyfikowany system bi-urojony* oparty na bazie $j\sqrt{2}$, w którym dopuszczono pewne modyfikacje dla zapewnienia reprezentacji wszystkich liczb ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=and%201%20is%20based%20on,used%20to%20encode%20each%20complex)). Konwersja do/z tej reprezentacji de facto sprowadzała się jednak do konwersji części rzeczywistej i urojonej na reprezentację w **ujemnej bazie -2** (czyli system *negabinary*) i traktowania ich osobno ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=%28%272N%20.%27.a%2Aa%2C.a,)). Innymi słowy, podobnie jak w poprzednich przypadkach, algorytmy arytmetyczne w dużej mierze redukowały się do operacji na tradycyjnych składnikach. **Mnożenie** dwóch liczb w systemie $j\sqrt{2}$, jak wykazano, składa się z kilku etapów obejmujących standardowe operacje: w istocie otrzymujemy konieczność wykonania standardowego mnożenia zespolonego (4 mnożenia rzeczywistych i 2 dodawania) oraz dodatkowego mnożenia rzeczywistego i dodawania zespolonego ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Z%2C%20%20Z%2C%20%3D%20Z%3B,imaginary%20complex)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=radix%20system%20the%20multiplication%20of,and%20one%20complex%20number%20addition)). Złożoność takiej operacji okazuje się **porównywalna z klasycznym algorytmem**, a brak redundancji w reprezentacji powoduje, że nie udaje się uzyskać uproszczenia względem przypadku 2’s complement ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=The%20conversion%20to%20and%20from,in%20this%20system%20is%20considered)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=radix%20system%20the%20multiplication%20of,and%20one%20complex%20number%20addition)). Można co prawda rozważyć także *redundantną* wersję systemu $j\sqrt{2}$ (z większym alfabetem cyfr), ale analogicznie jak przy $2j$, okaże się ona równoważna wykorzystaniu pewnej **wariacji systemu binarnego z osobną obsługą części rzeczywistej i urojonej** i nie przyniesie przewagi sprzętowej ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=For%20complex%20radix,and%20imaginary%20parts%20treated%20separately)).

**Ograniczenia:** Reprezentacja w bazie $j\sqrt{2}$ cierpi zatem na dwie główne bolączki: (1) **fraktalny charakter** przestrzeni liczb reprezentowalnych skończoną liczbą bitów – dla ustalonej długości słowa tylko pewne specyficzne kombinacje wartości realnej i urojonej dadzą się dokładnie przedstawić, a inne nie (są poza “siatką” reprezentacji, podobnie jak niektórych liczb nie da się dokładnie przedstawić w systemie binarnym o skończonej liczbie bitów, np. 1/10 w zapisie dwójkowym). (2) **Brak efektywności obliczeniowej** – podstawowe operacje arytmetyczne nie ulegają uproszczeniu; w istocie sprzętowa lub programowa realizacja musiałaby i tak rozkładać problem na operacje na częściach rzeczywistych/urojonych (np. mnożenie rozpisać na mnożenia składowych) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=The%20conversion%20to%20and%20from,in%20this%20system%20is%20considered)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=radix%20system%20the%20multiplication%20of,and%20one%20complex%20number%20addition)). Z tych powodów system $j\sqrt{2}$ jest głównie ciekawostką teoretyczną, a w praktyce (przy ograniczonej precyzji) jego **zastosowanie jest ograniczone** ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Further%2C%20many%20of%20the%20alternative,these%20alternative%20complex%20radices%20follows)).

### Klasyczna reprezentacja binarna (2’s complement, rozdzielone części)

**Zasada działania:** Klasyczna metoda reprezentacji liczb zespolonych w komputerach polega na **oddzielnym przechowywaniu części rzeczywistej i urojonej**, z których każda zapisana jest w typowym systemie dwójkowym ze znakiem (najczęściej w uzupełnieniu do dwóch). Na przykład 32-bitowa liczba zespolona może być przechowywana jako para 32-bitowych integerów (dla części rzeczywistej i urojonej) lub para liczb zmiennoprzecinkowych IEEE 754 (dla przybliżonej arytmetyki). W przypadku arytmetyki całkowitej na liczbach zespolonych, dodawanie wykonujemy poprzez niezależne dodanie dwóch 2’s complement (realnej z realną i urojonej z urojoną). Dla **mnożenia** stosujemy wzór algebraiczny:  
\[ (a + bi) \times (c + di) = (ac - bd) + (ad + bc)i, \]  
co przekłada się na cztery mnożenia liczb rzeczywistych oraz dwie operacje dodawania/odejmowania ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Z%2C%20%20Z%2C%20%3D%20Z%3B,imaginary%20complex)). Implementacja może być optymalizowana (np. wykorzystanie współdzielenia wyników, metoda Gaussa wymaga trzech mnożeń kosztem dodatkowych dodawań), ale generalnie rząd wielkości operacji pozostaje ten sam. 

**Zalety:** Największą zaletą klasycznej reprezentacji jest jej **prostota i wsparcie sprzętowe**. Ponieważ operujemy na standardowych liczbach binarnych (dla każdej części z osobna), możemy wykorzystać istniejące jednostki arytmetyczno-logiczne procesora. Każdy nowoczesny CPU potrafi szybko dodawać, mnożyć czy przesuwać liczby całkowite w 2’s complement – zatem operacje na liczbach zespolonych rozbijają się na zestaw takich właśnie operacji. Wysoka wydajność jest też wspierana przez kompilatory (które znają typy `complex<double>` itp.) i przez fakt, że dane są przechowywane w prosty sposób w pamięci (ciągłe bloki dla części realnej i urojonej, ewentualnie interleaved). Ponadto zakres wartości jest łatwy do kontrolowania – można osobno sprawdzać przepełnienia części rzeczywistej i urojonej. 

**Ograniczenia:** Klasyczne podejście *nie unifikuje* reprezentacji – w pamięci de facto wciąż mamy dwie liczby (dla części $A$ i $B$ w $A + Bi$). Powoduje to, że **logika operacji jest nieco bardziej złożona** (trzeba pamiętać o obu składowych). Przykładowo mnożenie wymaga kilku kroków i pośrednich wyników (cztery iloczyny i dwie sumy). Ponadto, aby przechować liczbę zespoloną, potrzeba dwukrotnie więcej pamięci niż na składową (choć to akurat nieuniknione w każdym systemie – informacja o dwóch stopniach swobody). W porównaniu z egzotycznymi reprezentacjami zespolonymi, klasyczna reprezentacja **wymaga bitu znaku** (w 2’s complement jest on niejako wbudowany w format) – co jednak w praktyce nie stanowi problemu, a stanowi wręcz zaletę (dzięki znakowi zakres jest symetryczny względem zera, w przeciwieństwie do systemów o ujemnej bazie bez znaku). 

W wielu zastosowaniach inżynierskich stosuje się też *dwuelementowe wektory* [re, im] liczb zmiennoprzecinkowych do reprezentacji liczb zespolonych. Operacje na nich wykorzystują jednostkę FPU CPU lub instrukcje wektorowe (SIMD). Również tam schemat jest klasyczny: obliczenia wykonuje się poprzez kombinacje operacji na częściach składowych.

### Podsumowanie porównania reprezentacji

Poniższa tabela zestawia kluczowe cechy omawianych reprezentacji liczbowych:

| **Reprezentacja**       | **Podstawa (radix)** | **Cyfry**        | **Zalety / Właściwości**                                                   | **Ograniczenia / Wady**                                                           |
|-------------------------|----------------------|------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| **Radix-(2j)**          | $2j$ (2 * i)         | {0, 1, 2, 3}     | – Każda liczba zespolona reprezentowana unikalnie **bez znaku** ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=As%20early%20as%201960%2C%20Knuth,2j%29%20which))<br>– Mnożenie/dzielenie możliwe “pisemnie” w jednej kolumnie, z **regułami przeniesień** dostosowanymi do podstawy ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=the%20conversion%20to%20and%20from,1%20two)). | – Wymaga **większej liczby cyfr** (cztery stany zamiast dwóch).<br>– **Niezrównoważony zakres** dla ustalonej liczby cyfr (trudność z reprezentacją maks/min) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Representing%20complex%20numbers%20in%20radix,4)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=Further%2C%20many%20of%20the%20alternative,these%20alternative%20complex%20radices%20follows)).<br>– Brak rzeczywistej przewagi nad konwencjonalnym systemem (po redukcji sprowadza się do bazy -4 osobno dla części re i im) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20summary%2C%20this%20analysis%20shows,a%20conventional%20binary%20number%20system)). |
| **Radix-(j-1)**         | $-1 + i$             | {0, 1}           | – Wykorzystuje **tylko cyfry binarne** 0 i 1 ([](https://tomrocksmaths.com/wp-content/uploads/2020/05/benjamin-chung.pdf#:~:text=There%20are%20two%20complex%20bases,every%20complex%20number%20that%20it)).<br>– Brak osobnego bitu znaku – **ujemne wartości reprezentują się same** dzięki właściwości podstawy. | – **Nietypowe przeniesienia** przy dodawaniu (możliwy przenoszony “bit” wpływający na dwie pozycje w lewo) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=%28l%29,l%2B%2C%20%2817)).<br>– Konieczność złożonej logiki do obsługi przeniesień (detektory zera, większa latencja) przy implementacji sumatora ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,additional%20latency%20and%20gate%20complexity)).<br>– Sprzętowo potencjalnie **wolniejsze operacje** niż klasyczne (większa sieć połączeń). |
| **Radix-(j√2)**         | $i \sqrt{2}$         | {0, 1}           | – **Cyfry binarne** 0/1 (teoretycznie prosty alfabet).<br>– Podstawa o module √2 daje większą “gęstość” potęg niż base-2 (więcej kombinacji z mniejszą liczbą cyfr, teoretycznie). | – Nie wszystkie liczby (np. **$i$**) mają skończony rozwinięcie – potrzebna **nieskończona liczba cyfr** ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=B.%20Radix,uses%20only%20the%20digits%200)).<br>– Mnożenie równie złożone jak klasyczne (wymaga w praktyce 4 mnożeń na składowych + dod.) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=radix%20system%20the%20multiplication%20of,and%20one%20complex%20number%20addition)).<br>– Wymaga specjalnych algorytmów konwersji (sprowadza się do negabinary dla części re/im) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=%28%272N%20.%27.a%2Aa%2C.a,)). |
| **Klasyczna (rozdzielna)** | $2$ (dla każdej części) | {0, 1} (2’s comp) | – **Prostota implementacji** – użycie istniejących arytmetyk binarnych.<br>– **Pełne wsparcie sprzętowe** w CPU/GPU (szybkie operacje na int/float).<br>– **Symetryczny zakres** (dzięki bitowi znaku, zakres od ujemnych do dodatnich równomiernie). | – **Rozdzielenie części** – operacje wymagają uwzględnienia dwóch składowych.<br>– Mnożenie wymaga **kilku operacji** (4 mnożenia i 2 dodania dla dwóch 32-bitowych składowych) – brak jednego zwartgo algorytmu.<br>– Zajmuje więcej pamięci (osobne słowa na Re i Im). |

*(Źródła: propozycje systemów: Knuth ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=As%20early%20as%201960%2C%20Knuth,2j%29%20which)), Penney ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=have%20been%20proposed.%20Knuth%20,1.%20Slekys%20%5B5%5D%20defined)), Slekys ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=computations%20in%20the%20radix,j%20f%20i)); analiza właściwości:  ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=the%20conversion%20to%20and%20from,1%20two)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,additional%20latency%20and%20gate%20complexity)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=B.%20Radix,uses%20only%20the%20digits%200)) i inne jak w tekście.)*

Jak wynika z powyższego porównania, **żaden z nietypowych systemów (radix-$2j$, $j-1$, $j\sqrt{2}$) nie daje zdecydowanej przewagi nad klasyczną reprezentacją** w kontekście prostoty i zakresu reprezentacji. Co więcej, literaturowe analizy wskazują, że w implementacjach sprzętowych nie osiąga się istotnej poprawy wydajności względem tradycyjnego 2’s complement ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=implies%20that%20the%20complex%20numbers,We%20find%20that%20no)). Mimo to, z edukacyjnego punktu widzenia warto zaimplementować i porównać te podejścia programowo, by lepiej zrozumieć ich działanie oraz **koszt obliczeniowy** podstawowych operacji (szczególnie mnożenia, które jest kluczowe w wielu algorytmach przetwarzania sygnałów). W kolejnych sekcjach przedstawione zostaną propozycje implementacji tych systemów oraz metody eksperymentalnego porównania ich wydajności.

## 2. Propozycje implementacji (Python i C++)

Implementacja każdego z powyższych systemów liczbowych wymaga zaprojektowania sposobu przechowywania liczby oraz algorytmów realizujących operacje arytmetyczne (przynajmniej dodawanie i mnożenie). Należy przy tym zwrócić uwagę na efektywność – zarówno czasową (optymalizacja kroków algorytmu), jak i pamięciową (reprezentacja danych). Poniżej omówiono możliwe podejścia implementacyjne dla każdego systemu, z uwzględnieniem specyfiki języków **Python** (wygodny dla prototypowania, obsługuje arbitralnie duże liczby całkowite) oraz **C++** (dający większą kontrolę nad niskopoziomową reprezentacją i wydajnością).

### Implementacja systemu radix-(2j)

**Reprezentacja danych:** Liczbę zespoloną w systemie o podstawie $2j$ możemy przechowywać jako **wektor (tablicę) jej cyfr** w tej podstawie. Każda cyfra może przyjmować wartości 0,1,2,3. W Pythonie najprostszym wyborem będzie lista liczb całkowitych (0–3), gdzie indeks 0 odpowiada cyfry przy $(2j)^0$, indeks 1 przy $(2j)^1$, itd. (lub odwrotnie – można przyjąć reprezentację od najbardziej znaczącej cyfry). W C++ można użyć np. `std::vector<int>` lub statycznej tablicy o ustalonym maksymalnym rozmiarze. Jeśli zakres wartości jest z góry ograniczony (np. do $n$ cyfr), można rozważyć zakodowanie takiej liczby w jednym słowie maszynowym: np. w 64-bitowym typie zmieści się do 32 cyfr (ponieważ 4 stany cyfr to równoważność 2 bitów informacji na cyfrę, 32*2 = 64 bity). Można zatem upakować dwie cyfry radix-$2j$ w jednym bajcie (8 bitów) albo zastosować bitowe operacje maskujące. Jednak dla prostoty implementacji i czytelności kodu zaleca się zacząć od struktury z tablicą cyfr.

**Algorytmy arytmetyczne:** Dodawanie dwóch liczb w tej reprezentacji wymaga zaimplementowania opisanych wcześniej reguł przeniesień. W Pythonie możemy to zrobić iterując po indeksach listy cyfr i sumując odpowiadające sobie cyfry $x_k + y_k + \text{ew. przeniesienie}$, po czym wynik dzielimy modulo 4 i aktualizujemy przeniesienie o $\pm1$ co **dwie** pozycje dalej (przeniesienie dodatnie przesuwamy w lewo o dwie pozycje gdy suma >=4, przeniesienie ujemne gdy suma < 0) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=allows%20the%20multiplication%20and%20division,two%20columns%20to%20the%20left)). Należy uważać, by właściwie aktualizować pozycję docelową przeniesienia – w praktyce można w tym celu korzystać z dodatkowej buforowej tablicy dwa razy dłuższej (by móc łatwo zapisać przeniesienie na pozycji $k+2$). W C++ implementacja będzie analogiczna – można użyć pętli for. Ważne jest zoptymalizowanie operacji na cyfrach: np. zamiast operacji dzielenia modulo 4 i warunków, można przygotować tabelkę decyzyjną lub wykorzystać fakt, że sumy i przeniesienia przyjmują niewiele stanów.

Mnożenie dwóch liczb w systemie $2j$ algorytmicznie przypomina mnożenie “pisemne” w systemie czwórkowym, z dodatkową logiką przeniesień. Podejście proste: dla każdej pary cyfr $(a_i$ z pierwszej liczby, $b_j$ z drugiej) obliczamy iloczyn $a_i \times b_j$ (to będzie zwykła liczba całkowita 0–9) i dodajemy do wyniku na pozycji $i+j$ (bo $(2j)^i \cdot (2j)^j = (2j)^{i+j}$). Tak jak przy mnożeniu w systemie dziesiętnym, możemy najpierw zapisać sumę częściową, a następnie wykonać normalizację/przeniesienia według reguł systemu $2j$. Alternatywnie można integrować przeniesienia w trakcie – to jednak dość skomplikowane. W implementacji programowej bezpieczniej jest: 
1. Wyznaczyć tymczasowy wynik jako dużą tablicę (długości sumy długości dwóch czynników) zainicjalizowaną zerami.
2. Dla każdej kombinacji cyfr dodać iloczyn do odpowiedniej pozycji.
3. Wykonać **normalizację** wyniku – przechodząc po tablicy wynikowej i redukując cyfry do zakresu 0–3, generując przeniesienia według reguł (przeniesienie o $\pm1$ co dwie pozycje).

Python jest wystarczająco elastyczny, by obsłużyć liczby z niemal dowolną liczbą cyfr (ograniczeniem jest pamięć i czas). W C++ dla efektywności warto rozważyć ograniczenie maksymalnej długości (np. projektujemy nasz system dla liczb mieszczących się w pewnym zakresie). Wtedy tablice mogą być statyczne lub alokowane na stosie, co przyspieszy operacje (unikanie alokacji dynamicznych). Można także wykorzystać **operator bitowy** do mnożenia w pewnych przypadkach: np. jeśli cyfry ograniczymy do 2 bitów, teoretycznie można by próbować użyć 64-bitowych mnożeń sprzętowych na upakowanych cyfrach. Jednak reguły przeniesienia co 2 pozycje komplikują bezpośrednie użycie wbudowanego mnożenia procesora – wynik mnożenia dwóch “upakowanych” liczb nie będzie od razu poprawny bez rozprzężenia przeniesień.

**Efektywność:** Python ma duży narzut interpretacji, więc implementacja w nim będzie kilkukrotnie wolniejsza niż analogiczna w C++. Jednak Python ma arbitralną precyzję liczb całkowitych i łatwość implementacji, co czyni go dobrym do szybkiego prototypu i weryfikacji poprawności. W C++ uzyskamy znacznie lepszą wydajność – szczególnie jeśli wykorzystamy niskopoziomowe optymalizacje (np. operacje bitowe, unroll pętli, SIMD do operacji na wektorach cyfr). Pamięciowo, przechowywanie $n$ cyfr w tablicy `int` w Pythonie ma narzut (każdy element listy to obiekt Pythona), więc alternatywnie można użyć tablicy modułu `array` lub `numpy` by przechować je ciągle w pamięci. W C++ typ `uint8_t` wystarczy do przechowania cyfry 0–3, więc można upakować 4 cyfry w jednym bajcie, ale operacje wtedy wymagają masek bitowych. Bardziej przejrzyste jest użycie `std::uint32_t` na każdą cyfrę (marnując nieco pamięci, ale upraszczając kod). Dla np. 16 cyfr to 16*32 = 512 bitów, co i tak jest niedużo.

Podsumowując, implementacja radix-$(2j)$ polega na napisaniu własnej mini-arytmetyki wielocyfrowej z niestandardową podstawą. Można ją traktować jak *framework* do testowania: najpierw stworzyć funkcje konwertujące zwykłą liczbę zespoloną (np. podaną jako para integerów) na reprezentację $2j$ (algorytm dzielenia przez $2j$ iteracyjnie), i odwrotnie – do celów weryfikacji. Następnie zaimplementować `add_2j(x,y)` i `mul_2j(x,y)` działające na wektorach cyfr zgodnie z regułami.

### Implementacja systemu radix-(j-1)

**Reprezentacja danych:** Liczby w systemie o podstawie $j-1$ są zapisywane bitowo (0/1). Zatem naturalnym wyborem w implementacji jest użycie **typów bitowych** lub struktur bitowych. W Pythonie można skorzystać z typu `int` wbudowanego (który przechowuje liczby binarnie, choć interpretujemy je tu inaczej). Możemy traktować zwykłą liczbę całkowitą Python jako ciąg bitów odpowiadających kolejnym współczynnikom przy potęgach $j-1$. Oczywiście w Pythonie operacje `+` czy `*` na takich intach **nie będą** respektować naszych reguł (będą zwykłym dodawaniem dziesiętnym), więc i tak musimy pisać własne procedury. Jednak przechowywanie bitów w jednym `int` jest efektywne pamięciowo i można uzyskać do nich dostęp przez operacje bitowe (maske, przesunięcia). Alternatywnie, można użyć typu `bitarray` lub po prostu listy bitów (0/1). W C++ mamy więcej opcji: `std::bitset<N>` jeśli znamy maksymalną liczbę bitów, lub dynamiczny `std::vector<bool>` (choć ten ma swoje ograniczenia wydajności). Możemy też użyć np. typu `unsigned long long` gdyby zakres był mały (np. do 64 bitów). W przypadku gdy potrzebujemy dowolnej długości, istnieją biblioteki multiprecision (Boost.Multiprecision BigInt) – wtedy można trochę oszukać i przechowywać wartość jako duży integer, ale znów standardowe operacje nie dadzą wyniku w naszym systemie. Lepiej więc jest operować bezpośrednio na bitach. W C++ bitowe przechowanie daje potencjał wysokiej wydajności (przetwarzanie 64 bitów naraz).

**Algorytmy arytmetyczne:** Dodawanie w systemie $j-1$ jest nietrywialne z powodu przeniesień skaczących o dwie pozycje. Trzeba zaimplementować algorytm podobny do sumatora z logiką wykrywania przeniesień. Prosty algorytm sekwencyjny może wyglądać tak: iterujemy od najmłodszego bitu do najstarszego:
- Sumujemy bit $x_k$, $y_k$ oraz **dwa** przeniesienia: jedno z pozycji $k-1$ (o ile takie istnieje) i jedno z pozycji $k-2$ (bo z dwóch pozycji w prawo mogą “doskoczyć” przeniesienia wygenerowane wcześniej). Na początku oczywiście nie ma przeniesień dla najmłodszych bitów.
- Na podstawie tej sumy decydujemy o wyniku na pozycji $k$ oraz generujemy przeniesienia *w lewo* (czyli na pozycje $k+1$ i $k+2$). 

Z uwagi na to, że przeniesienia mogą na siebie wzajemnie wpływać (jak opisano w sekcji 1, możliwe jest że jeden bit generuje przeniesienie o wartości 2 do dwóch kolejnych pozycji ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=%28l%29,l%2B%2C%20%2817))), implementacja iteracyjna musi być ostrożna, by nie nadpisać przeniesienia zanim zostanie uwzględnione. Można zachować tablicę pomocniczą dla przeniesień, gdzie np. `carry1[k]` oznacza przeniesienie docierające do pozycji $k$ z pozycji $k-2$, a `carry2[k]` z pozycji $k-1$. W każdym kroku sumowania, odczytujemy bieżące przeniesienia dla pozycji $k$, wyliczamy sumę bitów i generujemy nowe przeniesienia na wyższe pozycje. Ten algorytm może być implementowany w Pythonie wprost (w pętli) – powinien poprawnie zadziałać, choć potencjalnie jest wolny dla bardzo długich liczb (złożoność $O(n)$ dla dodawania $n$-bitowych liczb, co nie jest złe, ale stałe czasowe Pythona będą odczuwalne). 

W C++ można pokusić się o przyspieszenie: np. w przypadku braku przeniesień wynik dodawania to po prostu XOR bitów (jak dodawanie binarne bez przeniesień). Przeniesienia pojawiają się tylko dla pewnych kombinacji 1+1 itp. Można więc najpierw wyznaczyć sumę bitową i maski gdzie wystąpiły konflikty generujące przeniesienie, a potem iteracyjnie je rozpropagować. To trochę jak ogólny algorytm dodawania z przepływem przeniesień, tylko tu przeniesienia idą inaczej. Ewentualnie można opracować tablicę prawdy 4-wejściowej (x, y, carry_{k-1}, carry_{k-2}) -> (bit_wyniku, carry_{k+1}, carry_{k+2}) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=In%20radix,shown%20in%20the%20Table%20I)) i zaimplementować ją jako logikę bitową (na 64 równoległych bitach naraz). Tak właśnie zrobiono by w sprzęcie – np. generując sygnały przeniesień równolegle z sumą. W ramach projektu programistycznego jednak wystarczy zwykła implementacja krok po kroku, by zmierzyć jej działanie.

Mnożenie w systemie $j-1$ jest jeszcze bardziej złożone. Można zastosować podobną strategię jak dla $2j$: najpierw wykonujemy *iloczyn konwolucyjny* bitów (co sprowadza się do operacji AND między każdym bitem jednej liczby a każdym bitem drugiej, przesuniętych odpowiednio), a następnie sumujemy te wyniki uwzględniając reguły przeniesień. Innymi słowy, mnożenie dwóch $N$-bitowych liczb w tej reprezentacji da maksymalnie $2N$-bitowy wynik (jak w zwykłym systemie binarnym), ale proces sumowania tych $N^2$ częściowych iloczynów musi odbywać się według nietypowych zasad dodawania. To jest kosztowne obliczeniowo. W praktyce można zoptymalizować, korzystając z algorytmów mnożenia wielkich liczb (np. Karaatsuba, FFT) – lecz zaimplementowanie ich z customową arytmetyką przeniesień jest skomplikowane. W projekcie wystarczy zapewne zwykłe $O(n^2)$ mnożenie “podstawowe” dla umiarkowanych rozmiarów (np. do 64 bitów). 

**Efektywność:** W Pythonie reprezentacja bitowa (np. jako int) jest pamięciowo efektywna, ale nie możemy korzystać z natywnego dodawania. Implementując własne dodawanie bit po bicie, szybkość będzie ograniczona przez interpretera. Dla testów na niewielkich długościach (np. 32 bity) jest to akceptowalne. W C++ możemy osiągnąć dużo lepsze czasy: wykorzystanie 64-bitowych rejestrów do przetwarzania pozwoli w jednym kroku operować na 64 bitach danych równolegle. Potencjalnie, dla np. 128-bitowych liczb można skorzystać z typu __int128 (niestandardowy) lub biblioteki Boost.Multiprecision, ale te dadzą standardowe wyniki binarne, więc raczej pozostaniemy przy własnym kodzie. `std::bitset` ma przeciążone operatory AND, OR, XOR itp., co może się przydać. Pamięciowo, `std::vector<bool>` jest zoptymalizowany – wykorzystuje 1 bit na wartość – ale niestety dostęp do poszczególnych bitów jest tam zrealizowany przez proxy i jest wolniejszy niż np. operacja na `uint64_t`. Lepiej więc trzymać wewnętrznie np. `std::vector<uint64_t>` gdzie kolejne elementy reprezentują kolejne 64-bitowe “kawałki” liczby. Wtedy można manualnie obsłużyć przeniesienia między segmentami 64-bitowymi.

Reasumując, implementacja systemu $j-1$ to wyzwanie w poprawnym zaimplementowaniu algorytmów z nietypowymi przeniesieniami. Podejście warstwowe może pomóc: najpierw funkcje niskiego poziomu do operacji na bitach (ustaw, odczytaj bit, itp.), potem funkcja add, mul. **Testowanie** jest kluczowe (o czym w sekcji 3), bo łatwo o błąd w obsłudze skokowych przeniesień.

### Implementacja systemu radix-(j√2)

**Reprezentacja danych:** Podstawa $j\sqrt{2}$ również umożliwia użycie cyfr 0/1, więc z pozoru podobna jest do systemu $j-1$. Można by stosować ten sam model reprezentacji (bitowy). Jednak, jak wspomniano, nie wszystkie liczby dają się w ten sposób skończenie zapisać. Co to znaczy w praktyce implementacyjnej? Jeśli ograniczymy długość zapisu do $N$ cyfr, to reprezentujemy liczby w pewnym ograniczonym zbiorze (podobnie jak w systemie stałoprzecinkowym mamy ograniczony zakres i precyzję). Pojawi się więc kwestia **zaokrągleń** lub traktowania rozwinięć nieskończonych. Na potrzeby projektu można przyjąć, że operujemy na takich liczbach zespolonych, które *mają* skończone rozwinięcie w tym systemie dla rozsądnej liczby cyfr. Na przykład możemy ograniczyć się do Gaussian integers ($a+bi$ z całkowitymi $a,b$). Pytanie: czy każda taka liczba ma skończony zapis w base $i\sqrt{2}$? Jeśli nie (jak $i$), można wykluczyć pewne skrajne przypadki lub wprowadzić reprezentację ułamkową (np. dopuszczając pozycje po przecinku, czyli ujemne potęgi podstawy, co pozwoliłoby zapisać $i = 0.(10)_{i\sqrt{2}}$ może). W ramach prostoty można więc:
- Albo dopuścić **ujemne indeksy** (część ułamkową) w reprezentacji – wtedy np. $i = 1 \cdot (i\sqrt{2})^{-1}$ byłoby jednym bitem na pozycji -1.
- Albo ograniczyć dziedzinę liczb do takich, które mają skończony rozwój bez ułamków (np. wielokrotności pewnych liczb).

Pierwsze podejście (reprezentacja stałoprzecinkowa z punktem po pewnej liczbie cyfr) jest elastyczne – analogiczne do reprezentacji liczb rzeczywistych w stałej arytmetyce binarnej. Ustalmy np., że używamy $N$ bitów całkowitej części i $M$ bitów ułamkowych. Wtedy $i$ moglibyśmy reprezentować jako $0.1...0$ (1 na odpowiedniej pozycji ułamkowej). To wprowadza dodatkowe komplikacje (obsługa kropki), ale możliwe do implementacji: operacje jak dodawanie można robić na liczbach całkowitych przesuniętych o $M$ bitów (czyli traktować wszystko jako integer, tylko interpretować wynik).

Ze względu na złożoność, możliwe że w projekcie wystarczy pokazać implementację *podobną do $j-1$*, ewentualnie stwierdzając, że ograniczamy się do pewnego zbioru liczb. Reprezentacja bitowa (jak dla $j-1$) będzie wówczas punktem wyjścia.

**Algorytmy arytmetyczne:** Dodawanie w systemie $j\sqrt{2}$ rządzi się **innymi regułami przeniesień** niż w $j-1$. Slekys w swoich pracach przedstawił algorytmy arytmetyczne dla systemu bi-urojonego ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=The%20conversion%20to%20and%20from,in%20this%20system%20is%20considered)) ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=real%20and%20imaginary%20parts%20separately,in%20this%20system%20is%20considered)). Konwersja do ujemnej bazy -2 dla każdej części sugeruje, że dodawanie można sprowadzić do dodawania dwóch liczb w systemie negabinary (base -2) dla części rzeczywistych, osobno dla urojonych, a potem “sprzęgnąć” wynik. W praktyce można zaimplementować to bez takiej konwersji jawnie:
- Osobno dodajemy części rzeczywiste dwóch liczb (jako np. zwykłe integery) i osobno urojone (też jako integery).
- Jeśli którakolwiek z części przekroczy zakres lub wymaga przeniesienia “między” częścią rzeczywistą a urojoną, korygujemy to.

Trzeba tu przypomnieć, że $(i\sqrt{2})^2 = -2$ (rzeczywista liczba ujemna). Możliwe więc, że sumowanie dwóch liczb spowoduje np. że część urojona wygeneruje przeniesienie -2, które należy dodać do części rzeczywistej. Innymi słowy, przeniesienia mogą przechodzić **pomiędzy** częścią urojoną a rzeczywistą w tej reprezentacji. To dość niezwykłe – de facto powoduje, że sumowanie wymaga obserwowania wzajemnych zależności. Slekys proponował traktować to poprzez odseparowanie działań: najpierw normalizujemy część urojową i rzeczywistą w swoich bazach (-2), a potem dopiero łączymy wynik ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=The%20conversion%20to%20and%20from,in%20this%20system%20is%20considered)).

Mnożenie natomiast, jak wspomniano, nie daje uproszczeń – zapewne zaimplementujemy je tradycyjnie: używając klasycznego mnożenia zespolonego (4 mnożenia składowych) jako punktu wyjścia. Można ewentualnie sprawdzić, czy mnożąc *w reprezentacji bitowej* bez konwersji nie nastąpi coś prostszego, ale raczej nie – skończy się to bardzo złożoną logiką.

**Efektywność:** Z trzech nietypowych reprezentacji, $j\sqrt{2}$ jest najtrudniejsza do efektywnej implementacji, ze względu na konieczność obsługi rozwinięć ułamkowych lub ograniczania dziedziny. W Pythonie zapewne najprościej byłoby skorzystać z wysokiej precyzji i liczyć w prosty sposób (np. konwertując każdą liczbę do postaci $a+bi$, dodając normalnie i konwertując z powrotem na radiks – co traci cały sens porównania wydajności!). Dlatego, żeby być fair, implementację należy zrobić “ręcznie” jak dla $j-1$. W C++ podobnie – można wykorzystać pewne gotowe operacje, ale całość i tak będzie dość intensywna obliczeniowo.

W tym miejscu warto zauważyć, że jeśli celem projektu jest *porównanie efektywności obliczeniowej*, to reprezentacja $j\sqrt{2}$ prawdopodobnie wypadnie najgorzej (skomplikowane operacje + narzut obsługi ułamków lub bardzo długich słów dla przybliżeń). Implementując ją należy zadbać o poprawność, ale niekoniecznie optymalizować do granic – nawet mniej zoptymalizowana posłuży do zobrazowania różnicy.

### Implementacja klasycznej reprezentacji (2’s complement, rozdzielonej)

**Reprezentacja danych:** Tutaj możemy w pełni skorzystać z wbudowanych typów języka. W Pythonie mamy typ `complex`, ale on używa liczb zmiennoprzecinkowych – dla uczciwego porównania z powyższymi (które były raczej arytmetyką całkowitą/fiksoprzecinkową) lepiej użyć pary typu `int` lub np. własnej klasy. Można zdefiniować klasę `ComplexInt` z polami `re` i `im` typu int. W C++ można użyć `std::complex<int>` lub po prostu std::pair<int,int>. Obsługa uzupełnienia do 2 jest automatyczna w językach dla typów wbudowanych (np. przekroczenie zakresu 32-bit int spowoduje zawinięcie, ale możemy też kontrolować zakres samodzielnie jeśli chcemy symulować arbitralną wielkość – np. za pomocą biblioteki big integers).

**Algorytmy:** Dodawanie to osobno dodanie pól `re` i `im` (z ewentualnym sprawdzeniem overflow, jeśli symulujemy ograniczoną szerokość). Mnożenie – zgodnie ze wzorem: 
```cpp
res.re = a*c - b*d;
res.im = a*d + b*c;
``` 
gdzie `(a,b)` to składowe pierwszej, `(c,d)` drugiej liczby. W Pythonie można to samo zrobić operując na int. Kluczowe jest, by mierząc wydajność, nie wykorzystywać wbudowanej optymalizacji (np. Python `complex` byłby nieadekwatny, bo jest float). W C++ można też zaimplementować mnożenie nieco inaczej (np. używając mnożeń 64-bitowych jeśli 32-bitowe mogą się przepełnić, itp.), ale generalnie operacji będzie 4 mnożenia i 2 dodania.

**Efektywność:** W Pythonie operacje na małych int są szybkie (choć nie tak szybkie jak w C, ale implementowane w C wewnątrz Pythona). Dla rozsądnych zakresów nie będzie problemu z wydajnością. W C++ wydajność jest najwyższa z możliwych, bo korzystamy z natywnych instrukcji procesora. W pamięci para dwóch 32-bitowych liczb zajmuje 64 bity – co, jak wspomniano, jest i tak minimalnym kosztem, bo tyle informacji trzeba nieść. 

## 3. Metody porównania wydajności

Aby rzetelnie ocenić efektywność obliczeniową przedstawionych podejść, należy przygotować **plan testów i pomiarów**. W skład takiego planu powinny wchodzić:

- **Testy jednostkowe poprawności:** Przed mierzeniem szybkości, upewniamy się, że każda implementacja działa poprawnie. Należy przygotować zestaw testowych operacji (np. dodawanie i mnożenie kilkunastu wybranych par liczb) i porównać wyniki między różnymi reprezentacjami. Najprostszą metodą weryfikacji jest wykorzystanie klasycznej arytmetyki jako wzorca – np. losować dwie liczby zespolone $(a+bi), (c+di)$, obliczyć wynik mnożenia w sposób klasyczny, a następnie porównać z wynikiem funkcji `mul_2j`, `mul_j1`, `mul_jsqrt2`. Każda niezgodność oznacza błąd w implementacji. Testy jednostkowe można zautomatyzować (np. w Pythonie używając biblioteki `unittest` lub po prostu pętli `for` z porównaniami), w C++ np. poprzez asercje w pętli. Dobrze jest uwzględnić przypadki brzegowe (największe liczby dla danego rozmiaru, zera, jedynki, liczby ujemne itp.). 

- **Benchmarki czasowe:** Głównym celem jest pomiar czasu wykonywania operacji (zwłaszcza mnożenia) w różnych reprezentacjach. Należy zatem zaimplementować procedurę, która wykona dużą liczbę operacji i zmierzy łączny czas lub średni czas pojedynczej operacji. W Pythonie można użyć modułu `timeit` (np. wywołując `timeit.timeit` z odpowiednią lambdą wykonującą mnożenie) lub `perf_counter` z biblioteki `time` do pomiaru czasu przed i po serii operacji. Ważne, by **powtórzyć pomiar wielokrotnie** i wziąć np. medianę lub minimum z wielu prób (by zniwelować wpływ fluktuacji systemowych). W C++ można wykorzystać `<chrono>` do pobrania czasu wysokiej rozdzielczości przed i po pętli wykonującej np. milion mnożeń danej reprezentacji. Alternatywnie, istnieją frameworki (Google Benchmark) ułatwiające to – ale można i ręcznie. 

- **Różne scenariusze testowe:** Należy zdecydować, dla jakich danych będziemy mierzyć wydajność. Proponowane podejścia:
  - **Zmienna wielkość/liczba cyfr:** Sprawdzić, jak czas mnożenia rośnie wraz z “wielkością” liczb. Dla klasycznej reprezentacji wielkość to np. liczba bitów na część (8, 16, 32, 64 bit), a dla nietypowych reprezentacji – liczba cyfr w zapisie. Można wygenerować losowe liczby zespolone o rosnącym module (co zwykle pociąga za sobą więcej cyfr w tych systemach) i mnożyć je. 
  - **Operacje jednorazowe vs seryjne:** Sprawdzić koszt pojedynczego mnożenia oraz amortyzowany koszt w długiej serii. Np. może się okazać, że dla pewnych implementacji koszty jednorazowe (np. alokacja obiektów, konwersja danych) dominują, ale przy wielu operacjach stają się pomijalne.
  - **Dodawanie vs mnożenie:** Choć sedno to mnożenie, warto także zmierzyć czasy dodawania, aby potwierdzić teoretyczne przypuszczenia co do skali trudności (np. że dodawanie w $j-1$ jest już znacznie wolniejsze niż zwykłe, przez komplikacje z przeniesieniem).
  - **Różne języki/implementacje:** Można porównać implementację Pythonową z C++ dla tych samych danych, by zobaczyć różnicę wydajności wynikającą z języka. Jednak główny nacisk ma być na porównanie reprezentacji, więc zapewne wnioski skupią się na różnicach *między* systemami liczbowymi, a nie między językami per se.

- **Pomiary pamięciowe (opcjonalnie):** Jeśli istotne, można porównać ile pamięci zajmuje przechowanie tej samej liczby w różnych reprezentacjach. Np. liczba zespolona $Z = A + Bi$ zajmuje 64 bity w klasycznej reprezentacji (2×32b int). W systemie $2j$ trzeba policzyć ile cyfr wymaga zapis $A+Bi$ i pomnożyć przez rozmiar cyfry (np. 2 bity na cyfrę jeśli upakowane). Podobnie dla $j-1$ (to właściwie liczba bitów równa... tu pytanie jak długi jest zapis w najgorszym przypadku w porównaniu do binarnego – można zbadać). Takie porównanie może być zrobione teoretycznie i przedstawione w formie tabeli w raporcie (np. “dla zakresu do $2^{16}$ każdy ze systemów potrzebuje tylu a tylu cyfr/bitów”).

- **Analiza złożoności:** Niezależnie od pomiarów empirycznych, warto przeanalizować teoretyczną złożoność operacji. Dla klasycznego systemu mnożenie to $O(1)$ dla liczb stałej długości (np. 32-bit), ale $O(n^2)$ jeśli traktujemy n-bitowe liczby ogólnie (bo 4 mnożenia po $n$-bitów to pewne $O(n^2)$ operacji bitowych, choć można to poprawić algorytmami szybszymi). Dla systemu $2j$ mnożenie dwóch liczb o $n$ cyfrach to również $O(n^2)$ w najprostszej implementacji. Dodawanie to $O(n)$ dla wszystkich systemów (z różnymi stałymi). Można te zależności wspomnieć. Jeśli pomiary czasowe będą wykonywane dla zwiększających się $n$, dobrze jest zweryfikować czy wykres czasu od $n$ zgadza się z przewidywaniami (np. czy wykres jest kwadratowy). 

- **Prezentacja wyników:** Wyniki pomiarów najlepiej zaprezentować w formie **tabelarycznej** lub na wykresach. Np. tabela, gdzie w wierszach jest rozmiar liczby (ilość bitów/cyfr), a w kolumnach czas wykonania mnożenia dla każdej reprezentacji (Python i/lub C++). Jeśli projekt obejmuje przygotowanie raportu, można umieścić wykresy (np. log-log wykres czasu od wielkości operandu dla różnych metod). Ważne jest jednak, by opisy w tekście tłumaczyły te wyniki: np. “Widzimy, że dodawanie w systemie $j-1$ wypada około 5 razy wolniej od dodawania klasycznego dla 64-bitowych liczb, co jest spójne z koniecznością obsługi bardziej złożonych przeniesień. Mnożenie z kolei różni się o rzędy wielkości...”.

Podsumowując, metoda porównania wydajności polega na **zapewnieniu poprawności** implementacji, a następnie **zmierzeniu czasu** wykonywania typowych operacji (szczególnie mnożenia) w kontrolowanych warunkach. Należy pamiętać o wyeliminowaniu wpływu czynników zewnętrznych: testy powinny być powtarzalne, wykonywane na tym samym sprzęcie, najlepiej z wyłączonymi przełączeniami kontekstu (np. poprzez wykonywanie testu na wysokim priorytecie lub wielokrotne powtórzenia). W Pythonie trzeba uważać na *warm-up* (pierwsze wywołania mogą być wolniejsze ze względu na JIT cache CPU itp.), więc lepiej wykonać trochę “rozgrzewki” przed pomiarem.

## 4. Możliwości rozszerzenia funkcjonalności

Podczas realizacji projektu można rozważyć rozszerzenie zakresu funkcjonalności ponad podstawowe wymagania. Oto kilka propozycji takich rozszerzeń:

- **Różne rozmiary danych:** Jak wspomniano, warto zaimplementować obsługę różnych rozmiarów/liczb cyfr. Można pozwolić użytkownikowi klasy określić precyzję (np. 16-bit, 32-bit, 64-bit dla klasycznej reprezentacji albo maksymalną liczbę cyfr dla $2j$, $j-1$, $j\sqrt{2}$). Dzięki temu można badać zachowanie systemów zarówno w “niskiej rozdzielczości” (małe liczby, gdzie ewentualne narzuty są bardziej widoczne), jak i w bardzo wysokiej (gdzie asymptoty złożoności dadzą o sobie znać). W przypadku implementacji z dynamicznymi tablicami bitów/cyfr (Python list, C++ vector) zwiększenie rozmiaru to po prostu operacja na dłuższych listach. W przypadku użycia typów statycznych (bitset, int) można przygotować osobne klasy/szablony dla kilku ustalonych rozmiarów.

- **Konwersja między reprezentacjami:** Ciekawą funkcjonalnością (również weryfikującą zrozumienie tematu) jest zaimplementowanie metod konwersji **między różnymi systemami**. Na przykład, mając liczbę w reprezentacji radix-$2j$, możemy spróbować wygenerować jej równoważny zapis w systemie radix-$j-1$. To oczywiście sprowadza się do pewnych algorytmów matematycznych – najpierw konwersja $2j \to$ (a+bi) (czyli obliczenie rzeczywistej wartości liczby), a potem (a+bi) $\to j-1$. Ten drugi etap można zrealizować np. algorytmem dzielenia przez $j-1$: dopóki liczba nie jest zero, dzielimy ją przez $-1+i$ i zbieramy reszty 0/1 (to podobne do standardowego algorytmu dzielenia przez podstawę, z tym że tu dzielenie jest na liczbach zespolonych – ale dla Gaussian integers można to zrobić, wybierając odpowiedni iloraz zaokrąglony). Podobne konwersje można zrobić dla $j\sqrt{2}$ (z zastrzeżeniem, że pewne liczby nie wyjdą dokładnie – wtedy można np. obciąć rozwinięcie po ustalonej precyzji). Implementacja konwersji ma walor edukacyjny: pokazuje, jak liczby te “układają się” w różnych systemach i pozwala prześledzić przykłady konkretnych reprezentacji. Dla testera projektu możliwość przełączenia reprezentacji i sprawdzenia, że np. liczba zapisana jako `1011001` w base $j-1$ odpowiada konkretnej wartości, którą w base $2j$ zapisujemy jako `23...` itp., będzie potwierdzeniem zrozumienia.

- **Operacje na macierzach zespolonych:** Jeśli opanujemy już operacje na pojedynczych liczbach, można pójść krok dalej i zaimplementować działanie na macierzach (np. mnożenie macierzy 2x2 lub większych) z wykorzystaniem tych reprezentacji. W takim przypadku każda liczba w macierzy musiałaby być w jednym z systemów (można też mieszać, ale to sztuczne). Sprawdzenie mnożenia macierzy 2x2 to nic innego jak kilkukrotne mnożenie i dodawanie liczb zespolonych – więc będzie to bardziej test wydajnościowy złożonych operacji. Jednak może uwidocznić pewne efekty, np. sumowanie wielu składników w nietypowych systemach może kumulować opóźnienia (wielokrotne przeniesienia). Można porównać czas mnożenia macierzy klasycznie i np. w systemie $j-1$ dla macierzy różnych rozmiarów.

- **Obsługa sprzętowo-przyspieszona / wektorowa:** Bardziej zaawansowanym rozszerzeniem (raczej teoretycznym) może być próba wykorzystania instrukcji SIMD do przyspieszenia operacji. Np. w C++ przy użyciu intrinsics można załadować pewną liczbę cyfr do rejestru XMM/YMM i operować równolegle. Dla base $2j$ aż się prosi – bo cyfry to 2-bitowe wartości, więc 128-bitowy rejestr SSE mieści 64 takie cyfry i można by dodawać wektory cyfr równolegle, a następnie przesuwać przeniesienia. To jednak wymaga skomplikowanej logiki i wykracza poza typowy projekt studencki, ale można o tym wspomnieć jako o możliwości dalszego zwiększenia wydajności.

- **Wariacje systemów liczbowych:** Można także pokusić się o eksperyment z innymi podstawami zespolonymi spoza listy, np. $-1-i$ (który jest w zasadzie sprzężonym do $-1+i$ i powinien dać podobny efekt), albo dodać **system o podstawie ujemnej rzeczywistej** (negabinary, base -2) dla porównania – to nie jest liczba zespolona, ale negabinary ma cechy wspólne (też brak znaku i więcej przeniesień). Jednak to już dodatkowy wątek.

Ogólnie, rozszerzenia projektu powinny służyć **lepszemu zrozumieniu** tematu i pokazaniu pewnej elastyczności kodu. W praktycznej realizacji warto najpierw osiągnąć podstawową funkcjonalność (poprawne i w miarę wydajne dodawanie/mnożenie dla każdej reprezentacji), a dopiero potem dodawać kolejne bajery.

## 5. Struktura projektu i organizacja kodu

Dla czytelności i możliwości łatwego porównania poszczególnych podejść, projekt powinien być odpowiednio zorganizowany. Proponowana struktura to podział na następujące moduły/składniki:

- **Moduł logiki arytmetycznej** – zawierający implementacje reprezentacji i operacji. Można tutaj stworzyć osobne klasy dla każdej reprezentacji, np. `ComplexRadix2j`, `ComplexRadixJm1`, `ComplexRadixJsqrt2`, oraz ewentualnie `ComplexClassic`. Każda klasa enkapsuluje wewnętrzną reprezentację (np. wektor cyfr, albo parę intów) oraz udostępnia metody `add` i `mul` (oraz ewentualnie inne operatory). W języku **Python** można to zaimplementować jako osobne klasy z odpowiednimi metodami, a nawet przeciążyć operatory `__add__`, `__mul__` dla wygody. W **C++** można użyć klas lub struktur z metodami statycznymi/friend do operacji, albo nawet przeciążyć operator `+` i `*` dla tych klas. Istotne jest, by ten moduł nie zawierał kodu specyficznego dla testów czy pomiarów – tylko czystą logikę matematyczną. Dzięki temu łatwo będzie wielokrotnie wykorzystywać te funkcje.

- **Moduł testów jednostkowych** – część kodu (np. oddzielny plik), w której umieszczamy zestawy testowych danych i sprawdzamy poprawność wyników. Można tu wykorzystać asercje. Struktura może być prosta: pętle sprawdzające losowe przypadki, plus kilka na sztywno zdefiniowanych. Ważne jest oddzielenie tego od modułu logiki, aby ewentualne błędy łatwo lokalizować (jeśli test wychwyci rozbieżność, wiemy że problem w logice). W Pythonie moduł testowy mógłby importować klasy z modułu logiki i na końcu drukować “OK” gdy wszystko się zgadza. W C++ można to rozwiązać przez `assert` z `<cassert>` lub użyć frameworku (Google Test, Catch2) – ale to może być nadmiar, proste asercje wystarczą.

- **Moduł benchmarków (porównania wydajności)** – skrypt lub program, który wykonuje pomiary czasu. On także będzie korzystał z modułu logiki, wywołując np. mnożenie danej klasy w wielu iteracjach. Dobrze jest zaplanować interfejs tak, by łatwo można było dodać kolejną reprezentację do porównań. Na przykład, można napisać funkcję `benchmark_mul(complexType, data_samples)` która dla podanej klasy kompleksowej wykona mnożenia na elementach z listy `data_samples`. W Pythonie zamiast typów można przekazywać np. instancje lub funkcje. W C++ można użyć szablonu lub funktorów. Kluczowe jest, by wszystkie testy wykonały porównywalną pracę. Ten moduł powinien zebrać wyniki i np. wypisać je w formacie tabelarycznym (który potem można skopiować do raportu) albo nawet wygenerować plik CSV do późniejszej analizy.

- **Skrypt główny / interfejs użytkownika (opcjonalnie)** – jeśli planujemy, by użytkownik (np. osoba sprawdzająca projekt) mógł interaktywnie wpisać własne liczby i zobaczyć wyniki operacji, można stworzyć prosty interfejs. W Pythonie może to być po prostu część main z input() i print(), w C++ analogicznie `main()` czytający argumenty. Można umożliwić wybór trybu (testy, benchmarki, demo operacji). Nie jest to konieczne, ale bywa mile widziane.

- **Dokumentacja i komentarze:** Kod powinien być opatrzony komentarzami wyjaśniającymi kluczowe fragmenty (zwłaszcza nietrywialne pętle przeniesień itp.). Dodatkowo, raport (taki jak ten) służy jako dokumentacja wyższego poziomu. W strukturze projektu można dostarczyć plik README z opisem jak uruchomić testy/benchmarki.

- **Podział na pliki (dla C++):** Można każdy reprezentację umieścić w osobnym pliku nagłówkowym/źródłowym, lub wszystkie klasy w jednym pliku nagłówkowym `complex_radix.h` dla prostoty. Testy i main w oddzielnych plikach `.cpp`. W Pythonie analogicznie: moduł np. `complex_radix.py` z definicjami klas, i np. `test_complex.py` oraz `benchmark_complex.py`.

Taka struktura zapewni **separację warstw** – warstwa logiki arytmetycznej jest niezależna i można ją testować w izolacji, warstwa testowa dba o poprawność, a warstwa porównawcza o zbieranie wyników. Dzięki temu również w raporcie łatwo wykazać, że rozumiemy temat: osobno omawiamy **co** implementujemy (logika), a osobno **jak sprawdzamy** i **porównujemy** wyniki (testy, benchmarki).

Na zakończenie projektu, w raporcie, można dodać krótkie omówienie wyników porównań. Prawdopodobnie okaże się, że klasyczna reprezentacja 2’s complement jest najbardziej wydajna dla większości operacji (co zgadza się z oczekiwaniami i literaturą ([COMPLEX-1-REG5-2004-113-complex-numbers.pdf](file://file-AsbM8S6FFNd6uWvYeBKDUP#:~:text=implies%20that%20the%20complex%20numbers,We%20find%20that%20no))), natomiast reprezentacje nietypowe wypadają wolniej – ale dzięki praktycznej implementacji student lepiej zrozumie **dlaczego** tak jest, mając wgląd w mechanizmy przeniesień i złożoność arytmetyki. Projekt zrealizowany w ten sposób nie tylko dostarcza wyników liczbowych, ale przede wszystkim pogłębia intuicję na temat arytmetyki zespolonej i systemów liczbowych.